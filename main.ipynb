{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by standardizing the \"By H. P. Lovecraft\" text:\n",
    "def clean_title(title):\n",
    "    author_text = \"By H. P. Lovecraft\"\n",
    "    title = re.sub(rf\"({author_text}\\s*)+\", author_text, title).strip()\n",
    "    if title.endswith(author_text) and not title.endswith(\" \" + author_text):\n",
    "        title = title.replace(author_text, \" \" + author_text)\n",
    "    return title\n",
    "\n",
    "# Scraping Lovecraft Fiction:\n",
    "\n",
    "def scrape_lovecraft_content():\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith('fiction/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = 'data/lovecraft_fiction.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    author_tag = content_soup.find('font', size=\"+1\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = f\"{title_tag.get_text(strip=True)} by {author_tag.get_text(strip=True)}\"\n",
    "                        title = clean_title(title)  # Clean the title text\n",
    "                        csvwriter.writerow([\"fiction\", title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "scrape_lovecraft_content()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scraped CSV data into DF:\n",
    "df = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# View first few rows:\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords:\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize and remove stopwords:\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply to text column in DF:\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create frequency distribution for words:\n",
    "all_words = \" \".join(df['Cleaned_Text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display most common words:\n",
    "print(word_freq.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract entities:\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'LOC']]\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for Entity Recognition:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of Lovecraftian entities:\n",
    "lovecraftian_entities = [\n",
    "    \"cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"azathoth\", \"hastur\", \"r'lyeh\", \"dagon\", \n",
    "    \"shub-niggurath\", \"the great old ones\", \"elder gods\", \"the old ones\", \"the deep ones\", \"night gaunts\", \n",
    "    \"cthulhu cult\", \"the nameless city\", \"the black stone\", \"the dreamlands\", \"fenric\", \"hecuba\", \n",
    "    \"animus\", \"tor-gasukk\", \"moloch\", \"kai'lizakia\", \"lloigor\", \"eidolon\", \"derleth\", \"gog\", \"magog\", \"to'koth\", \n",
    "    \"karnas'koi\", \"traguam\", \"archon\", \"mi'en kalarash\", \"kwundaar\", \"volund\", \"k'thun\", \"noth-yidik\", \"tru'nembra\", \n",
    "    \"tulzscha\", \"cxaxukluth\", \"d'endrrah\", \"ubbo-sathla\", \"xexanoth\", \"ycnàgnnisssz\", \"yhoundeh\", \"aiueb gnshal\", \n",
    "    \"aletheia\", \"azhorra-tha\", \"c'thalpa\", \"daoloth\", \"ghroth\", \"gi-hoveg\", \"haiogh-yai\", \"huitloxopetl\", \"ialdagorth\", \n",
    "    \"kaajh'kaalbh\", \"kaalut\", \"lu-kthu\", \"mh'ithrha\", \"mlandoth\", \"mril thorion\", \"mother of pus\", \"nhimbaloth\", \n",
    "    \"ngyr-khorath\", \"nyctelios\", \"olkoth\", \"ramasekva\", \"shabbith-ka\", \"star mother\", \"suc'naath\", \"uvhash\", \"xa'ligha\", \n",
    "    \"yibb-tstll\", \"yidhra\", \"yomag'n'tho\"\n",
    "]\n",
    "\n",
    "# Indirect references or variations (these should be generalized and defined outside the loop for efficiency)\n",
    "indirect_references = [\n",
    "    r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "    r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "    r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\"\n",
    "]\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "\n",
    "    # Manually add expanded entities and regex pattern matches for direct references\n",
    "    for entity in lovecraftian_entities:\n",
    "        # Check for direct entity mentions (singular and plural)\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "        \n",
    "        # Check for indirect references or variations\n",
    "        for pattern in indirect_references:\n",
    "            if re.search(pattern, text.lower()):\n",
    "                entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraftian entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraftian_entities}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the mentions of each entity across all rows in 'Entities' column\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_mentions = Counter(all_entities)\n",
    "\n",
    "# Convert the Counter object to a list of tuples (entity, mentions)\n",
    "entities = [(entity, count) for entity, count in entity_mentions.items()]\n",
    "\n",
    "# Function to filter and categorize entities:\n",
    "def filter_lovecraft_entities(entities, exclude_humans=True):\n",
    "\n",
    "    # Dictionary to store entities by category:\n",
    "    categories = {\n",
    "        'Cthulhu Mythos': [],\n",
    "        'Locations & Settings': [],\n",
    "        'Cosmic Entities': [],\n",
    "        'Occult Entities': [],\n",
    "        'Mythos-Related Concepts': []\n",
    "    }\n",
    "\n",
    "    # Known classifications:\n",
    "    mythos_deities = {'cthulhu', 'great cthulhu', 'nyarlathotep', 'azathoth', 'shub-niggurath', \n",
    "                      'dagon', 'yog-sothoth', 'hastur', 'ubbo-sathla', 'ghroth', 'ycnàgnnisssz'}\n",
    "    \n",
    "    locations = {'arkham', 'miskatonic', 'innsmouth', 'dunwich', \"r'lyeh\", 'the dreamlands', 'the nameless city'}\n",
    "    \n",
    "    cosmic_entities = {'elder gods', 'the old ones', 'the great old ones', 'night gaunts', 'the deep ones', \n",
    "                       'colour out of space', 'yog sothoth', 'tulzscha', 'cxaxukluth', 'yhoundeh', \n",
    "                       'aiueb gnshal', 'aletheia', 'azhorra-tha', \"c'thalpa\", 'daoloth', 'ghroth', 'gi-hoveg', \n",
    "                       'haiogh-yai', 'huitloxopetl', 'ialdagorth', \"kaajh'kaalbh\", 'kaalut', 'lu-kthu', \n",
    "                       \"mh'ithrha\", 'mlandoth', 'mril thorion', 'mother of pus', 'nhimbaloth', \n",
    "                       'ngyr-khorath', 'nyctelios', 'olkoth', 'ramasekva', 'shabbith-ka', 'star mother', \n",
    "                       \"suc'naath\", 'uvhash', \"xa'ligha\", 'yibb-tstll', 'yidhra', \"yomag'n'tho\"}\n",
    "    \n",
    "    occult_entities = {'toymakers', 'guardians of time', 'the great intelligence', 'moloch', \n",
    "                       'hecuba', 'animus', 'archon', 'kwundaar', 'volund', 'noth-yidik', 'tru\\'nembra'}\n",
    "\n",
    "    # Categorize entities:\n",
    "    for entity, mentions in entities:\n",
    "\n",
    "        if entity in mythos_deities:\n",
    "            categories['Cthulhu Mythos'].append((entity, mentions))\n",
    "        elif entity in locations:\n",
    "            categories['Locations & Settings'].append((entity, mentions))\n",
    "        elif entity in cosmic_entities:\n",
    "            categories['Cosmic Entities'].append((entity, mentions))\n",
    "        elif entity in occult_entities:\n",
    "            categories['Occult Entities'].append((entity, mentions))\n",
    "        else:\n",
    "            categories['Mythos-Related Concepts'].append((entity, mentions))\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Apply filter:\n",
    "filtered_entities = filter_lovecraft_entities(entities)\n",
    "\n",
    "# Print result:\n",
    "for category, entities_list in filtered_entities.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for entity, mentions in entities_list:\n",
    "        print(f\"{entity}: {mentions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count mentions of each entity across all rows in Entities column:\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_mentions = Counter(all_entities)\n",
    "\n",
    "# Convert Counter object to DF (top 10):\n",
    "entity_df = pd.DataFrame(entity_mentions.most_common(10), columns=['Entity', 'Frequency'])\n",
    "\n",
    "# Plot top 10 entities:\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequency', y='Entity', data=entity_df, palette='viridis', hue='Entity')\n",
    "plt.title('Top 10 Lovecraftian Entities')\n",
    "plt.xlabel('Frequency of Mentions')\n",
    "plt.ylabel('Entity')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
