{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by standardizing the \"By H. P. Lovecraft\" text:\n",
    "def clean_title(title):\n",
    "    author_text = \"By H. P. Lovecraft\"\n",
    "    title = re.sub(rf\"({author_text}\\s*)+\", author_text, title).strip()\n",
    "    if title.endswith(author_text) and not title.endswith(\" \" + author_text):\n",
    "        title = title.replace(author_text, \" \" + author_text)\n",
    "    return title\n",
    "\n",
    "# --- Step 1: Scraping Lovecraft Fiction Works ---\n",
    "\n",
    "def scrape_lovecraft_content():\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith('fiction/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = 'data/lovecraft_fiction.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    author_tag = content_soup.find('font', size=\"+1\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = f\"{title_tag.get_text(strip=True)} by {author_tag.get_text(strip=True)}\"\n",
    "                        title = clean_title(title)  # Clean the title text\n",
    "                        csvwriter.writerow([\"fiction\", title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "# Scrape only fiction content:\n",
    "scrape_lovecraft_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped CSV data into a Pandas DataFrame\n",
    "df = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# View the first few rows of the dataframe to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply to the text column in the dataframe\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a frequency distribution for words in all texts\n",
    "all_words = \" \".join(df['Cleaned_Text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the most common words\n",
    "print(word_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a word cloud from the cleaned text\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(df['Cleaned_Text']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC']]\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom list of Lovecraft entities to track\n",
    "lovecraft_entities = [\n",
    "    \"cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"innsmouth\", \"arkham\", \"dunwich\", \"azathoth\", \"shub-niggurath\",\n",
    "    \"hastur\", \"miskatonic\", \"the king in yellow\", \"the dark young\", \"the colour out of space\", \"the great old ones\"\n",
    "]\n",
    "\n",
    "# Extract frequencies for these specific entities\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraft_entities}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Expanded list of Lovecraftian and related entities (including new ones)\n",
    "lovecraft_entities_expanded = [\n",
    "    \"cthulhu\", \"great cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"innsmouth\", \"arkham\", \"dunwich\", \"azathoth\",\n",
    "    \"shub-niggurath\", \"hastur\", \"miskatonic\", \"the king in yellow\", \"the dark young\", \"colour out of space\",\n",
    "    \"the great old ones\", \"elder gods\", \"the old ones\", \"the deep ones\", \"night gaunts\", \"cthulhu cult\",\n",
    "    \"the nameless city\", \"the black stone\", \"the dreamlands\", \"r'lyeh\", \"yog sothoth\", \"nyarlathotep\",\n",
    "    \"dunwich horror\", \"the whisperer in darkness\", \"the colour out of space\", \"the shadow over innsmouth\", \"dagon\",\n",
    "    \"the great intelligence\", \"fenric\", \"nestene consciousness\", \"guardians of time\", \"celestial toymaker\", \n",
    "    \"gods of ragnarok\", \"hecuba\", \"animus\", \"tor-gasukk\", \"moloch\", \"kai'lizakia\", \"lloigor\", \"toymakers\",\n",
    "    \"eidolon\", \"derleth\", \"gog and magog\", \"to'koth\", \"karnas'koi\", \"traguam\", \"archon\", \"mi'en kalarash\", \n",
    "    \"kwundaar\", \"volund\"\n",
    "]\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "    \n",
    "    # Extract the named entities using spaCy's NER (PERSON, ORG, GPE, LOC)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC']:\n",
    "            entities.append(ent.text.lower())\n",
    "    \n",
    "    # Manually add expanded entities and regex pattern matches for indirect references\n",
    "    for entity in lovecraft_entities_expanded:\n",
    "        # Check for direct entity mentions (singular and plural)\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "        \n",
    "        # Check for indirect references or variations (e.g., \"great old ones\" or \"eldritch horror\")\n",
    "        indirect_references = [\n",
    "            r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "            r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "            r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\"\n",
    "        ]\n",
    "        for pattern in indirect_references:\n",
    "            if re.search(pattern, text.lower()):\n",
    "                entities.append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraft entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraft_entities_expanded}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of entities with their mentions\n",
    "entities = [\n",
    "    ('cthulhu', 21), ('great cthulhu', 17), ('yog-sothoth', 12), ('nyarlathotep', 40),\n",
    "    ('innsmouth', 17), ('arkham', 73), ('dunwich', 13), ('azathoth', 17), ('shub-niggurath', 12),\n",
    "    ('hastur', 13), ('miskatonic', 26), ('the king in yellow', 12), ('the dark young', 12),\n",
    "    ('colour out of space', 12), ('the great old ones', 12), ('elder gods', 16), ('the old ones', 12),\n",
    "    ('the deep ones', 12), ('night gaunts', 12), ('cthulhu cult', 14), ('the nameless city', 12),\n",
    "    ('the black stone', 12), ('the dreamlands', 12), (\"r'lyeh\", 12), ('yog sothoth', 12),\n",
    "    ('dunwich horror', 13), ('the whisperer in darkness', 12), ('the colour out of space', 12),\n",
    "    ('the shadow over innsmouth', 12), ('dagon', 16), ('the great intelligence', 12), ('fenric', 12),\n",
    "    ('nestene consciousness', 12), ('guardians of time', 12), ('celestial toymaker', 12),\n",
    "    ('gods of ragnarok', 12), ('hecuba', 12), ('animus', 12), ('tor-gasukk', 12), ('moloch', 13),\n",
    "    (\"kai'lizakia\", 12), ('lloigor', 12), ('toymakers', 12), ('eidolon', 17), ('derleth', 13),\n",
    "    ('gog and magog', 12), (\"to'koth\", 12), (\"karnas'koi\", 12), ('traguam', 12), ('archon', 13),\n",
    "    (\"mi'en kalarash\", 12), ('kwundaar', 12), ('volund', 12), ('joseph curwen', 53), ('boston', 51),\n",
    "    ('whateley', 45), ('clarendon', 35), ('london', 33), ('new york', 32), ('africa', 26), \n",
    "    ('johnny', 20), ('paris', 19), ('ammi', 19), ('denis', 19), ('europe', 17), ('wilbur', 14),\n",
    "    ('ben', 14), ('dobson', 14), ('california', 13), ('mexico city', 13), ('yog-sothoth', 12),\n",
    "    ('shub-niggurath', 12), ('the king in yellow', 12), ('the dark young', 12), ('colour out of space', 12),\n",
    "    ('the great old ones', 12), ('the old ones', 12), ('the deep ones', 12), ('night gaunts', 12),\n",
    "    ('the nameless city', 12), ('the black stone', 12), ('the dreamlands', 12), (\"r'lyeh\", 12), \n",
    "    ('yog sothoth', 12), ('the whisperer in darkness', 12), ('the colour out of space', 12), \n",
    "    ('the shadow over innsmouth', 12), ('the great intelligence', 12), ('fenric', 12), \n",
    "    ('nestene consciousness', 12), ('guardians of time', 12), ('celestial toymaker', 12), \n",
    "    ('gods of ragnarok', 12), ('hecuba', 12), ('animus', 12), ('tor-gasukk', 12), (\"kai'lizakia\", 12),\n",
    "    ('lloigor', 12), ('toymakers', 12), ('eidolon', 17), ('derleth', 13), ('gog and magog', 12),\n",
    "    (\"to'koth\", 12), (\"karnas'koi\", 12), ('traguam', 12), ('archon', 13), (\"mi'en kalarash\", 12),\n",
    "    ('kwundaar', 12), ('volund', 12)\n",
    "]\n",
    "\n",
    "# List of human names to exclude\n",
    "human_names = ['joseph curwen', 'johnny', 'ammi', 'denis', 'wilbur', 'ben', 'dobson', 'george campbell',\n",
    "               'steve', 'dalton', 'joe slater', 'randolph carter', 'john brown', 'de marigny', 'wilbur whateley']\n",
    "\n",
    "# Function to filter and categorize entities\n",
    "def filter_lovecraft_entities(entities, exclude_humans=True):\n",
    "    # Create a dictionary to store entities by category\n",
    "    categories = {\n",
    "        'Cthulhu Mythos': [],\n",
    "        'Locations & Settings': [],\n",
    "        'Cosmic Entities': [],\n",
    "        'Occult Entities': [],\n",
    "        'Mythos-Related Concepts': []\n",
    "    }\n",
    "    \n",
    "    # Process each entity in the list\n",
    "    for entity, mentions in entities:\n",
    "        # Exclude human characters\n",
    "        if exclude_humans and entity.lower() in human_names:\n",
    "            continue\n",
    "        \n",
    "        # Categorize entities based on their known groupings\n",
    "        if entity in ['cthulhu', 'great cthulhu', 'nyarlathotep', 'azathoth', 'shub-niggurath', 'dagon', 'yog-sothoth']:\n",
    "            categories['Cthulhu Mythos'].append((entity, mentions))\n",
    "        elif entity in ['arkham', 'miskatonic', 'innsmouth', 'dunwich', 'r\\'lyeh', 'the dreamlands', 'the nameless city']:\n",
    "            categories['Locations & Settings'].append((entity, mentions))\n",
    "        elif entity in ['elder gods', 'the old ones', 'the great old ones', 'night gaunts', 'the deep ones', 'colour out of space', 'yog sothoth']:\n",
    "            categories['Cosmic Entities'].append((entity, mentions))\n",
    "        elif entity in ['toymakers', 'guardians of time', 'the great intelligence', 'moloch', 'hecuba', 'animus', 'archon']:\n",
    "            categories['Occult Entities'].append((entity, mentions))\n",
    "        else:\n",
    "            categories['Mythos-Related Concepts'].append((entity, mentions))\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# Apply the filter\n",
    "filtered_entities = filter_lovecraft_entities(entities)\n",
    "\n",
    "# Print the result\n",
    "for category, entities_list in filtered_entities.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for entity, mentions in entities_list:\n",
    "        print(f\"{entity}: {mentions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
