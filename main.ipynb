{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by removing author names:\n",
    "def clean_title(title):\n",
    "    # Remove any mention of \"by\" or \"with\" along with author names (e.g., \"by H. P. Lovecraft\")\n",
    "    title = re.sub(r'(by\\s*H\\.?\\s*P\\.?\\s*Lovecraft|with\\s*H\\.?\\s*P\\.?\\s*Lovecraft)', '', title, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Remove any other remaining author mentions (like \"By C. M. Eddy, Jr.\" etc.)\n",
    "    title = re.sub(r'(by\\s+[a-zA-Z\\.\\,]+)', '', title, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Remove extra spaces or redundant punctuation\n",
    "    title = ' '.join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "# Scraping Lovecraft Fiction:\n",
    "def scrape_lovecraft_content():\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith('fiction/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = 'data/lovecraft_fiction.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = title_tag.get_text(strip=True)\n",
    "                        title = clean_title(title)  # Clean the title to remove authors\n",
    "                        csvwriter.writerow([\"fiction\", title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "scrape_lovecraft_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scraped CSV data into DF:\n",
    "df = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# View first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Count number of words per story:\n",
    "df['Word Count'] = df['Text'].apply(lambda text: len([word for word in text.split() if word.isalpha()]))\n",
    "\n",
    "# View updated DF with word counts:\n",
    "print(df[['Title', 'Word Count']].head())\n",
    "\n",
    "# 1. Calculate Text Length (Character Counts)\n",
    "df['Text Length'] = df['Text'].apply(len)\n",
    "\n",
    "# 2. Perform Sentiment Analysis (polarity)\n",
    "def get_sentiment(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    # Return the sentiment polarity (-1 to 1)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to each story\n",
    "df['Sentiment'] = df['Text'].apply(get_sentiment)\n",
    "\n",
    "# View the dataframe with Text Length and Sentiment columns\n",
    "print(df[['Title', 'Text Length', 'Sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Word Count, Text Length (Character Count), and Sentiment, then get the frequency of each combination\n",
    "grouped_df = df.groupby(['Title', 'Word Count', 'Text Length', 'Sentiment']).size().reset_index(name='Frequency')\n",
    "\n",
    "# View the head and tail of the grouped dataframe\n",
    "print(\"Head of the grouped dataframe:\")\n",
    "print(grouped_df.head())\n",
    "\n",
    "print(\"\\nTail of the grouped dataframe:\")\n",
    "print(grouped_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total word count of all texts\n",
    "total_word_count = df['Word Count'].sum()\n",
    "\n",
    "# Print total word count\n",
    "print(f\"\\nTotal Word Count Across All Texts: {total_word_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Title, Word Count, Text Length, and Sentiment, then get the frequency of each combination\n",
    "grouped_df = df.groupby(['Title', 'Word Count', 'Text Length', 'Sentiment']).size().reset_index(name='Frequency')\n",
    "\n",
    "# Calculate the average word count\n",
    "average_word_count = df['Word Count'].mean()\n",
    "\n",
    "# Find the title with the fewest and most words\n",
    "min_word_count = df['Word Count'].min()\n",
    "max_word_count = df['Word Count'].max()\n",
    "\n",
    "min_word_title = df[df['Word Count'] == min_word_count][['Title', 'Word Count']]\n",
    "max_word_title = df[df['Word Count'] == max_word_count][['Title', 'Word Count']]\n",
    "\n",
    "# Find the highest and lowest sentiment scores\n",
    "min_sentiment = df['Sentiment'].min()\n",
    "max_sentiment = df['Sentiment'].max()\n",
    "\n",
    "min_sentiment_title = df[df['Sentiment'] == min_sentiment][['Title', 'Sentiment']]\n",
    "max_sentiment_title = df[df['Sentiment'] == max_sentiment][['Title', 'Sentiment']]\n",
    "\n",
    "# View the head and tail of the grouped dataframe\n",
    "print(\"Head of the grouped dataframe:\")\n",
    "print(grouped_df.head())\n",
    "\n",
    "print(\"\\nTail of the grouped dataframe:\")\n",
    "print(grouped_df.tail())\n",
    "\n",
    "# Print word count statistics\n",
    "print(f\"\\nAverage Word Count: {average_word_count:.2f}\")\n",
    "print(f\"\\nFewest Words: {min_word_count} (Title: {min_word_title.iloc[0]['Title']})\")\n",
    "print(f\"Most Words: {max_word_count} (Title: {max_word_title.iloc[0]['Title']})\")\n",
    "\n",
    "# Print sentiment score statistics\n",
    "print(f\"\\nLowest Sentiment Score: {min_sentiment} (Title: {min_sentiment_title.iloc[0]['Title']})\")\n",
    "print(f\"Highest Sentiment Score: {max_sentiment} (Title: {max_sentiment_title.iloc[0]['Title']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total word count of all texts\n",
    "total_word_count = df['Word Count'].sum()\n",
    "\n",
    "# Print total word count\n",
    "print(f\"\\nTotal Word Count Across All Texts: {total_word_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your df has the following columns: 'Word Count', 'Sentiment', and 'Text'\n",
    "# If not, make sure to adjust as per your dataset\n",
    "\n",
    "# Set up the figure for all plots to be displayed in one cell\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Average Word Count Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=df['Word Count'])\n",
    "plt.axvline(df['Word Count'].mean(), color='red', linestyle='--', label=f'Average: {df[\"Word Count\"].mean():.2f}')\n",
    "plt.title('Distribution of Word Counts in Lovecraft’s Fiction')\n",
    "plt.xlabel('Word Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Sentiment Score Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df['Sentiment'], kde=True, color='purple', bins=20)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot 3: Sentiment vs. Word Count (Bivariate plot)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x='Word Count', y='Sentiment', data=df, color='orange')\n",
    "plt.title('Sentiment Score vs. Word Count')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "# Show all plots together\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "\n",
    "# 1. **Average Word Count Distribution**:\n",
    "# The first plot shows the distribution of word counts in the dataset. The red dashed line indicates the average word count.\n",
    "# From this plot, you can observe whether most stories are around the average word count or if there are any extreme values. \n",
    "# For example, if there are many outliers, it suggests that some stories are much longer or shorter than the rest.\n",
    "# This information is useful for understanding the typical length of Lovecraft’s stories and spotting anomalies.\n",
    "\n",
    "# 2. **Sentiment Score Distribution**:\n",
    "# The second plot presents the distribution of sentiment scores. The use of a KDE (Kernel Density Estimate) gives us a smooth curve to visualize the overall sentiment.\n",
    "# If the sentiment score is skewed toward positive or negative values, it could indicate that most stories have a certain mood (e.g., negative or neutral).\n",
    "# This visualization is helpful to understand the general sentiment across all stories. If there's a sharp peak at one end, it suggests a dominant tone in the dataset.\n",
    "\n",
    "# 3. **Sentiment vs. Word Count**:\n",
    "# The scatter plot shows how sentiment and word count are related. It’s important to see if longer stories tend to have a more positive or negative sentiment. \n",
    "# For instance, if longer stories cluster towards positive sentiment, it might suggest that Lovecraft's longer works have a more optimistic tone, or vice versa.\n",
    "# This bivariate analysis is useful for identifying trends or correlations between the length of stories and their emotional tone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson correlation between Word Count and Sentiment Score:\n",
    "correlation, p_value = pearsonr(df['Word Count'], df['Sentiment'])\n",
    "print(f\"Pearson's correlation coefficient: {correlation:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the p-value:\n",
    "if p_value < 0.05:\n",
    "    print(\"The correlation is statistically significant.\")\n",
    "else:\n",
    "    print(\"The correlation is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords:\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize and remove stopwords:\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply to text column in DF:\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distribution for words:\n",
    "all_words = \" \".join(df['Cleaned_Text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display most common words:\n",
    "print(word_freq.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for Entity Recognition:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of Lovecraftian entities:\n",
    "lovecraftian_entities = [\n",
    "    'cthulhu', 'nyarlathotep', 'azathoth', 'hastur', 'dagon', 'shub-niggurath', 'tor-gasukk', \n",
    "    'yog-sothoth', 'moloch', \"kai'lizakia\", 'lloigor', 'derleth', 'gog', 'magog', \"to'koth\", \n",
    "    \"karnas'koi\", 'traguam', \"mi'en kalarash\", 'kwundaar', 'volund', \"k'thun\", 'noth-yidik', \n",
    "    \"tru'nembra\", 'tulzscha', 'cxaxukluth', \"d'endrrah\", 'ubbo-sathla', 'xexanoth', 'ycnàgnnisssz',\n",
    "    'yhoundeh', 'aiueb gnshal', 'aletheia', 'azhorra-tha', \"c'thalpa\", 'daoloth', 'ghroth', 'gi-hoveg', \n",
    "    'haiogh-yai', 'huitloxopetl', 'ialdagorth', \"kaajh'kaalbh\", 'kaalut', 'lu-kthu', \"mh'ithrha\", \n",
    "    'mlandoth', 'mril thorion', 'nhimbaloth', 'ngyr-khorath', 'nyctelios', 'olkoth', 'ramasekva', \n",
    "    'shabbith-ka', 'suc\\'naath', 'uvhash', \"xa'ligha\", 'yibb-tstll', 'yidhra', \"yomag'n'tho\"\n",
    "]\n",
    "\n",
    "# Indirect references or variations (to catch general or implied references)\n",
    "indirect_references = [\n",
    "    r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "    r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "    r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\", r\"\\bcult of cthulhu\\b\", r\"\\bancient god\\b\", r\"\\bbeyond time\\b\",\n",
    "    r\"\\beldritch horror\\b\", r\"\\bunspeakable entity\\b\", r\"\\bforbidden knowledge\\b\", r\"\\bunknown terror\\b\",\n",
    "    r\"\\bchaos god\\b\", r\"\\bblind idiot god\\b\", r\"\\bblind demon\\b\", r\"\\bblind idiot\\b\", r\"\\bblind god\\b\",\n",
    "    r\"\\bstar spawn\\b\", r\"\\bstar gods\\b\", r\"\\bstar beings\\b\", r\"\\bstar creatures\\b\", r\"\\bstar entities\\b\",\n",
    "    r\"\\bancient ones\\b\", r\"\\belders\\b\", r\"\\bgreat old ones\\b\", r\"\\bgreat ones\\b\", r\"\\botherworldly beings\\b\",\n",
    "    r\"\\bcosmic power\\b\", r\"\\beldritch entity\\b\", r\"\\bforbidden deity\\b\", r\"\\botherworldly presence\\b\",\n",
    "    r\"\\bhorrific power\\b\", r\"\\botherworldly evil\\b\", r\"\\bancient forces\\b\", r\"\\bunnamable god\\b\",\n",
    "    r\"\\bforbidden knowledge\\b\", r\"\\bshapeless horror\\b\", r\"\\bhorrible mind\\b\", r\"\\bbeyond comprehension\\b\",\n",
    "    r\"\\botherworldly force\\b\", r\"\\bunfathomable entity\\b\", r\"\\bvoid entity\\b\", r\"\\bhorrific eldritch power\\b\",\n",
    "    r\"\\bdark unknown force\\b\", r\"\\bprimordial chaos\\b\", r\"\\bprimordial entity\\b\", r\"\\bmaddening entity\\b\",\n",
    "    r\"\\bdivine horror\\b\", r\"\\bdark presence\\b\", r\"\\bcosmic horror\\b\", r\"\\bgreat nightmare\\b\",\n",
    "    r\"\\bhorrible night\\b\", r\"\\bgod of the void\\b\", r\"\\bmaddening mind\\b\", r\"\\bthe void\\b\", r\"\\bbeast of darkness\\b\"\n",
    "]\n",
    "\n",
    "\n",
    "# Function to extract entities, both direct and indirect\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "\n",
    "    # Check for direct entity mentions (singular and plural)\n",
    "    for entity in lovecraftian_entities:\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "    \n",
    "    # Check for indirect references or variations\n",
    "    for pattern in indirect_references:\n",
    "        if re.search(pattern, text.lower()):\n",
    "            # Add all entities if a matching indirect reference is found\n",
    "            entities.extend(lovecraftian_entities)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraftian entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraftian_entities}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Your new list of entities with updated counts\n",
    "entities = [\n",
    "    ('cthulhu', 46), ('nyarlathotep', 45), ('azathoth', 42), ('dagon', 40), ('hastur', 38), \n",
    "    ('moloch', 38), ('derleth', 38), ('shub-niggurath', 37), ('tor-gasukk', 37), ('yog-sothoth', 37),\n",
    "    (\"kai'lizakia\", 37), ('lloigor', 37), ('gog', 37), ('magog', 37), (\"to'koth\", 37), (\"karnas'koi\", 37), \n",
    "    ('traguam', 37), (\"mi'en kalarash\", 37), ('kwundaar', 37), ('volund', 37), (\"k'thun\", 37), ('noth-yidik', 37), \n",
    "    (\"tru'nembra\", 37), ('tulzscha', 37), ('cxaxukluth', 37), (\"d'endrrah\", 37), ('ubbo-sathla', 37), ('xexanoth', 37),\n",
    "    ('ycnàgnnisssz', 37), ('yhoundeh', 37), ('aiueb gnshal', 37), ('aletheia', 37), ('azhorra-tha', 37),\n",
    "    (\"c'thalpa\", 37), ('daoloth', 37), ('ghroth', 37), ('gi-hoveg', 37), ('haiogh-yai', 37),\n",
    "    ('huitloxopetl', 37), ('ialdagorth', 37), (\"kaajh'kaalbh\", 37), ('kaalut', 37), ('lu-kthu', 37),\n",
    "    (\"mh'ithrha\", 37), ('mlandoth', 37), ('mril thorion', 37), ('nhimbaloth', 37), ('ngyr-khorath', 37),\n",
    "    ('nyctelios', 37), ('olkoth', 37), ('ramasekva', 37), ('shabbith-ka', 37), ('suc\\'naath', 37),\n",
    "    ('uvhash', 37), (\"xa'ligha\", 37), ('yibb-tstll', 37), ('yidhra', 37), (\"yomag'n'tho\", 37)\n",
    "]\n",
    "\n",
    "# Function to filter and categorize entities:\n",
    "def filter_lovecraft_entities(entities):\n",
    "    # Dictionary to store entities by category:\n",
    "    categories = {\n",
    "        'Cthulhu Mythos': [],\n",
    "        'Cosmic Entities': [],\n",
    "    }\n",
    "\n",
    "    # Known classifications:\n",
    "    mythos_deities = {'cthulhu', 'nyarlathotep', 'azathoth', 'shub-niggurath', \n",
    "                      'dagon', 'yog-sothoth', 'hastur', 'ubbo-sathla', 'ghroth', 'ycnàgnnisssz'}\n",
    "    \n",
    "    cosmic_entities = {'tulzscha', 'cxaxukluth', 'yhoundeh', \n",
    "                       'aiueb gnshal', 'aletheia', 'azhorra-tha', \"c'thalpa\", 'daoloth', 'ghroth', 'gi-hoveg', \n",
    "                       'haiogh-yai', 'huitloxopetl', 'ialdagorth', \"kaajh'kaalbh\", 'kaalut', 'lu-kthu', \n",
    "                       \"mh'ithrha\", 'mlandoth', 'mril thorion', 'nhimbaloth', 'moloch', 'kwundaar', 'volund', 'noth-yidik', 'tru\\'nembra'\n",
    "                       'ngyr-khorath', 'nyctelios', 'olkoth', 'ramasekva', 'shabbith-ka', \"suc'naath\", 'uvhash', \"xa'ligha\", 'yibb-tstll', 'yidhra', \"yomag'n'tho\"}\n",
    "    \n",
    "    # Categorize entities:\n",
    "    for entity, mentions in entities:\n",
    "\n",
    "        if entity in mythos_deities:\n",
    "            categories['Cthulhu Mythos'].append((entity, mentions))\n",
    "        elif entity in cosmic_entities:\n",
    "            categories['Cosmic Entities'].append((entity, mentions))\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Apply filter:\n",
    "filtered_entities = filter_lovecraft_entities(entities)\n",
    "\n",
    "# Print result:\n",
    "for category, entities_list in filtered_entities.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for entity, mentions in entities_list:\n",
    "        print(f\"{entity}: {mentions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count mentions of each entity across all rows in Entities column:\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_mentions = Counter(all_entities)\n",
    "\n",
    "# Convert Counter object to DF (top 10):\n",
    "entity_df = pd.DataFrame(entity_mentions.most_common(20), columns=['Entity', 'Frequency'])\n",
    "\n",
    "# Capitalize the first letter of each word in the entity names:\n",
    "entity_df['Entity'] = entity_df['Entity'].apply(lambda x: x.title())\n",
    "\n",
    "# Plot top 10 entities:\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequency', y='Entity', data=entity_df, palette='viridis', hue='Entity')\n",
    "plt.title('Top 10 Lovecraftian Entities')\n",
    "plt.xlabel('Frequency of Mentions')\n",
    "plt.ylabel('Entity')\n",
    "\n",
    "# Set the x-axis to show integer values without decimals:\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words (word frequency analysis)\n",
    "top_words = word_freq.most_common(20)\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y=list(words), x=list(counts))  # Swapping x and y\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the most common entities\n",
    "top_entities = entity_freq.most_common(20)\n",
    "entities, entity_counts = zip(*top_entities)\n",
    "\n",
    "# Capitalize the first letter of each word in the entity names:\n",
    "entities = [entity.title() for entity in entities]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y=list(entities), x=list(entity_counts))  # Swapping x and y\n",
    "plt.title('Top 20 Most Common Lovecraftian Entities')\n",
    "plt.ylabel('Entities')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "# Set the x-axis to show integer values without decimals:\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
