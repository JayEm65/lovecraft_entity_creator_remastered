{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by standardizing the \"By H. P. Lovecraft\" text:\n",
    "def clean_title(title):\n",
    "    author_text = \"By H. P. Lovecraft\"\n",
    "    title = re.sub(rf\"({author_text}\\s*)+\", author_text, title).strip()\n",
    "    if title.endswith(author_text) and not title.endswith(\" \" + author_text):\n",
    "        title = title.replace(author_text, \" \" + author_text)\n",
    "    return title\n",
    "\n",
    "# --- Step 1: Scraping Lovecraft Fiction Works ---\n",
    "\n",
    "def scrape_lovecraft_content():\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith('fiction/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = 'data/lovecraft_fiction.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    author_tag = content_soup.find('font', size=\"+1\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = f\"{title_tag.get_text(strip=True)} by {author_tag.get_text(strip=True)}\"\n",
    "                        title = clean_title(title)  # Clean the title text\n",
    "                        csvwriter.writerow([\"fiction\", title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "# Scrape only fiction content:\n",
    "scrape_lovecraft_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped CSV data into a Pandas DataFrame\n",
    "df = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# View the first few rows of the dataframe to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply to the text column in the dataframe\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a frequency distribution for words in all texts\n",
    "all_words = \" \".join(df['Cleaned_Text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the most common words\n",
    "print(word_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC']]\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Expanded list of Lovecraftian and related entities (including new ones)\n",
    "lovecraft_entities_expanded = [\n",
    "    \"cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"azathoth\", \"hastur\", \"r'lyeh\", \"dagon\", \n",
    "    \"shub-niggurath\", \"the great old ones\", \"elder gods\", \"the old ones\", \"the deep ones\", \"night gaunts\", \n",
    "    \"cthulhu cult\", \"the nameless city\", \"the black stone\", \"the dreamlands\", \"fenric\", \"hecuba\", \n",
    "    \"animus\", \"tor-gasukk\", \"moloch\", \"kai'lizakia\", \"lloigor\", \"eidolon\", \"derleth\", \"gog\", \"magog\", \"to'koth\", \n",
    "    \"karnas'koi\", \"traguam\", \"archon\", \"mi'en kalarash\", \"kwundaar\", \"volund\", \"k'thun\", \"noth-yidik\", \"tru'nembra\", \n",
    "    \"tulzscha\", \"cxaxukluth\", \"d'endrrah\", \"ubbo-sathla\", \"xexanoth\", \"ycn√†gnnisssz\", \"yhoundeh\", \"aiueb gnshal\", \n",
    "    \"aletheia\", \"azhorra-tha\", \"c'thalpa\", \"daoloth\", \"ghroth\", \"gi-hoveg\", \"haiogh-yai\", \"huitloxopetl\", \"ialdagorth\", \n",
    "    \"kaajh'kaalbh\", \"kaalut\", \"lu-kthu\", \"mh'ithrha\", \"mlandoth\", \"mril thorion\", \"mother of pus\", \"nhimbaloth\", \n",
    "    \"ngyr-khorath\", \"nyctelios\", \"olkoth\", \"ramasekva\", \"shabbith-ka\", \"star mother\", \"suc'naath\", \"uvhash\", \"xa'ligha\", \n",
    "    \"yibb-tstll\", \"yidhra\", \"yomag'n'tho\"\n",
    "]\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "    \n",
    "    # Extract the named entities using spaCy's NER (PERSON, ORG, GPE, LOC)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC']:\n",
    "            entities.append(ent.text.lower())\n",
    "    \n",
    "    # Manually add expanded entities and regex pattern matches for indirect references\n",
    "    for entity in lovecraft_entities_expanded:\n",
    "        # Check for direct entity mentions (singular and plural)\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "        \n",
    "        # Check for indirect references or variations (e.g., \"great old ones\" or \"eldritch horror\")\n",
    "        indirect_references = [\n",
    "            r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "            r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "            r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\"\n",
    "        ]\n",
    "        for pattern in indirect_references:\n",
    "            if re.search(pattern, text.lower()):\n",
    "                entities.append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraft entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraft_entities_expanded}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Expanded list of Lovecraftian and related entities (including new ones)\n",
    "lovecraft_entities_expanded = [\n",
    "    \"cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"azathoth\", \"hastur\", \"r'lyeh\", \"dagon\", \n",
    "    \"shub-niggurath\", \"the great old ones\", \"elder gods\", \"the old ones\", \"the deep ones\", \"night gaunts\", \n",
    "    \"cthulhu cult\", \"the nameless city\", \"the black stone\", \"the dreamlands\", \"fenric\", \"hecuba\", \n",
    "    \"animus\", \"tor-gasukk\", \"moloch\", \"kai'lizakia\", \"lloigor\", \"eidolon\", \"derleth\", \"gog\", \"magog\", \"to'koth\", \n",
    "    \"karnas'koi\", \"traguam\", \"archon\", \"mi'en kalarash\", \"kwundaar\", \"volund\", \"k'thun\", \"noth-yidik\", \"tru'nembra\", \n",
    "    \"tulzscha\", \"cxaxukluth\", \"d'endrrah\", \"ubbo-sathla\", \"xexanoth\", \"ycn√†gnnisssz\", \"yhoundeh\", \"aiueb gnshal\", \n",
    "    \"aletheia\", \"azhorra-tha\", \"c'thalpa\", \"daoloth\", \"ghroth\", \"gi-hoveg\", \"haiogh-yai\", \"huitloxopetl\", \"ialdagorth\", \n",
    "    \"kaajh'kaalbh\", \"kaalut\", \"lu-kthu\", \"mh'ithrha\", \"mlandoth\", \"mril thorion\", \"mother of pus\", \"nhimbaloth\", \n",
    "    \"ngyr-khorath\", \"nyctelios\", \"olkoth\", \"ramasekva\", \"shabbith-ka\", \"star mother\", \"suc'naath\", \"uvhash\", \"xa'ligha\", \n",
    "    \"yibb-tstll\", \"yidhra\", \"yomag'n'tho\"\n",
    "]\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "    \n",
    "    # Manually add expanded entities and regex pattern matches for direct references\n",
    "    for entity in lovecraft_entities_expanded:\n",
    "        # Check for direct entity mentions (singular and plural)\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "        \n",
    "        # Check for indirect references or variations (e.g., \"great old ones\" or \"eldritch horror\")\n",
    "        indirect_references = [\n",
    "            r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "            r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "            r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\"\n",
    "        ]\n",
    "        for pattern in indirect_references:\n",
    "            if re.search(pattern, text.lower()):\n",
    "                entities.append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraft entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraft_entities_expanded}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of entities with their mentions\n",
    "entities = [\n",
    "     ('cthulhu', 21), ('nyarlathotep', 20), ('azathoth', 17), ('eidolon', 17), ('elder gods', 16), ('dagon', 15), \n",
    "     ('cthulhu cult', 14), ('hastur', 13), ('moloch', 13), ('derleth', 13), ('archon', 13), ('yog-sothoth', 12), \n",
    "     (\"r'lyeh\", 12), ('shub-niggurath', 12), ('the great old ones', 12), ('the old ones', 12), ('the deep ones', 12), \n",
    "     ('night gaunts', 12), ('the nameless city', 12), ('the black stone', 12), ('the dreamlands', 12), ('fenric', 12), \n",
    "     ('hecuba', 12), ('animus', 12), ('tor-gasukk', 12), (\"kai'lizakia\", 12), ('lloigor', 12), ('gog', 12), ('magog', 12), \n",
    "     (\"to'koth\", 12), (\"karnas'koi\", 12), ('traguam', 12), (\"mi'en kalarash\", 12), ('kwundaar', 12), ('volund', 12), \n",
    "     (\"k'thun\", 12), ('noth-yidik', 12), (\"tru'nembra\", 12), ('tulzscha', 12), ('cxaxukluth', 12), (\"d'endrrah\", 12), \n",
    "     ('ubbo-sathla', 12), ('xexanoth', 12), ('ycn√†gnnisssz', 12), ('yhoundeh', 12), ('aiueb gnshal', 12), ('aletheia', 12), \n",
    "     ('azhorra-tha', 12), (\"c'thalpa\", 12), ('daoloth', 12), ('ghroth', 12), ('gi-hoveg', 12), ('haiogh-yai', 12), \n",
    "     ('huitloxopetl', 12), ('ialdagorth', 12), (\"kaajh'kaalbh\", 12), ('kaalut', 12), ('lu-kthu', 12), (\"mh'ithrha\", 12), \n",
    "     ('mlandoth', 12), ('mril thorion', 12), ('mother of pus', 12), ('nhimbaloth', 12), ('ngyr-khorath', 12), ('nyctelios', 12), \n",
    "     ('olkoth', 12), ('ramasekva', 12), ('shabbith-ka', 12), ('star mother', 12), (\"suc'naath\", 12), ('uvhash', 12), \n",
    "     (\"xa'ligha\", 12), ('yibb-tstll', 12), ('yidhra', 12), (\"yomag'n'tho\", 12)\n",
    "]\n",
    "\n",
    "# Function to filter and categorize entities\n",
    "def filter_lovecraft_entities(entities, exclude_humans=True):\n",
    "    # List of human names to be excluded\n",
    "    human_names = ['carter', 'willett', 'joseph curwen', 'johnny', 'ammi', 'denis', \n",
    "                   'wilbur', 'ben', 'dobson', 'george campbell', 'fed', 'van der', 'steve', \n",
    "                   'dalton', 'joe slater', 'nahum', 'davis', 'wilbur whateley', 'joe', \n",
    "                   'john', 'nova', 'romero', 'robert grandison', 'joe mazurewicz', 'warren', \n",
    "                   'joe', 'arthur jermyn', 'jermyn house', 'joe', 'arthur munroe', 'james dalton', \n",
    "                   'anderson', 'robert suydam', 'herbert west', 'sefton', 'matt', 'miller', \n",
    "                   'arthur wheeler', 'robert', 'henry akeley', 'norman']\n",
    "    \n",
    "    # Create a dictionary to store entities by category\n",
    "    categories = {\n",
    "        'Cthulhu Mythos': [],\n",
    "        'Locations & Settings': [],\n",
    "        'Cosmic Entities': [],\n",
    "        'Occult Entities': [],\n",
    "        'Mythos-Related Concepts': []\n",
    "    }\n",
    "    \n",
    "    # Process each entity in the list\n",
    "    for entity, mentions in entities:\n",
    "        # Exclude human characters\n",
    "        if exclude_humans and entity.lower() in human_names:\n",
    "            categories['Humanoid Names'].append((entity, mentions))\n",
    "            continue\n",
    "        \n",
    "        # Categorize entities based on their known groupings\n",
    "        if entity in ['cthulhu', 'great cthulhu', 'nyarlathotep', 'azathoth', 'shub-niggurath', 'dagon', 'yog-sothoth']:\n",
    "            categories['Cthulhu Mythos'].append((entity, mentions))\n",
    "        elif entity in ['arkham', 'miskatonic', 'innsmouth', 'dunwich', 'r\\'lyeh', 'the dreamlands', 'the nameless city']:\n",
    "            categories['Locations & Settings'].append((entity, mentions))\n",
    "        elif entity in ['elder gods', 'the old ones', 'the great old ones', 'night gaunts', 'the deep ones', 'colour out of space', 'yog sothoth']:\n",
    "            categories['Cosmic Entities'].append((entity, mentions))\n",
    "        elif entity in ['toymakers', 'guardians of time', 'the great intelligence', 'moloch', 'hecuba', 'animus', 'archon']:\n",
    "            categories['Occult Entities'].append((entity, mentions))\n",
    "        else:\n",
    "            categories['Mythos-Related Concepts'].append((entity, mentions))\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# Apply the filter\n",
    "filtered_entities = filter_lovecraft_entities(entities)\n",
    "\n",
    "# Print the result\n",
    "for category, entities_list in filtered_entities.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for entity, mentions in entities_list:\n",
    "        print(f\"{entity}: {mentions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
