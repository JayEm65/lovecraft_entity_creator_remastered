{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by removing author names:\n",
    "def clean_title(title):\n",
    "    # Remove any mention of \"by\" or \"with\" along with author names (e.g., \"by H. P. Lovecraft\")\n",
    "    title = re.sub(r'(by\\s*H\\.?\\s*P\\.?\\s*Lovecraft|with\\s*H\\.?\\s*P\\.?\\s*Lovecraft)', '', title, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Remove any other remaining author mentions (like \"By C. M. Eddy, Jr.\" etc.)\n",
    "    title = re.sub(r'(by\\s+[a-zA-Z\\.\\,]+)', '', title, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Remove extra spaces or redundant punctuation\n",
    "    title = ' '.join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "# Scraping Lovecraft Fiction:\n",
    "def scrape_lovecraft_content():\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith('fiction/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = 'data/lovecraft_fiction.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = title_tag.get_text(strip=True)\n",
    "                        title = clean_title(title)  # Clean the title to remove authors\n",
    "                        csvwriter.writerow([\"fiction\", title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "scrape_lovecraft_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load scraped CSV data into DF:\n",
    "df = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# View first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Count the number of words per story (excluding commas and other punctuation)\n",
    "df['Word Count'] = df['Text'].apply(lambda text: len([word for word in text.split() if word.isalpha()]))\n",
    "\n",
    "# View the updated dataframe with word counts\n",
    "print(df[['Title', 'Word Count']].head())\n",
    "\n",
    "# 1. Calculate Text Length (Character Counts)\n",
    "df['Text Length'] = df['Text'].apply(len)\n",
    "\n",
    "# 2. Perform Sentiment Analysis (polarity)\n",
    "def get_sentiment(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    # Return the sentiment polarity (-1 to 1)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to each story\n",
    "df['Sentiment'] = df['Text'].apply(get_sentiment)\n",
    "\n",
    "# View the dataframe with Text Length and Sentiment columns\n",
    "print(df[['Title', 'Text Length', 'Sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your df has the following columns: 'Word Count', 'Sentiment', and 'Text'\n",
    "# If not, make sure to adjust as per your dataset\n",
    "\n",
    "# Set up the figure for all plots to be displayed in one cell\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Average Word Count Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=df['Word Count'])\n",
    "plt.axvline(df['Word Count'].mean(), color='red', linestyle='--', label=f'Average: {df[\"Word Count\"].mean():.2f}')\n",
    "plt.title('Distribution of Word Counts in Lovecraft’s Fiction')\n",
    "plt.xlabel('Word Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Sentiment Score Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df['Sentiment'], kde=True, color='purple', bins=20)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot 3: Sentiment vs. Word Count (Bivariate plot)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x='Word Count', y='Sentiment', data=df, color='orange')\n",
    "plt.title('Sentiment Score vs. Word Count')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "# Show all plots together\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "\n",
    "# 1. **Average Word Count Distribution**:\n",
    "# The first plot shows the distribution of word counts in the dataset. The red dashed line indicates the average word count.\n",
    "# From this plot, you can observe whether most stories are around the average word count or if there are any extreme values. \n",
    "# For example, if there are many outliers, it suggests that some stories are much longer or shorter than the rest.\n",
    "# This information is useful for understanding the typical length of Lovecraft’s stories and spotting anomalies.\n",
    "\n",
    "# 2. **Sentiment Score Distribution**:\n",
    "# The second plot presents the distribution of sentiment scores. The use of a KDE (Kernel Density Estimate) gives us a smooth curve to visualize the overall sentiment.\n",
    "# If the sentiment score is skewed toward positive or negative values, it could indicate that most stories have a certain mood (e.g., negative or neutral).\n",
    "# This visualization is helpful to understand the general sentiment across all stories. If there's a sharp peak at one end, it suggests a dominant tone in the dataset.\n",
    "\n",
    "# 3. **Sentiment vs. Word Count**:\n",
    "# The scatter plot shows how sentiment and word count are related. It’s important to see if longer stories tend to have a more positive or negative sentiment. \n",
    "# For instance, if longer stories cluster towards positive sentiment, it might suggest that Lovecraft's longer works have a more optimistic tone, or vice versa.\n",
    "# This bivariate analysis is useful for identifying trends or correlations between the length of stories and their emotional tone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute Pearson correlation between Word Count and Sentiment Score\n",
    "correlation, p_value = pearsonr(df['Word Count'], df['Sentiment'])\n",
    "print(f\"Pearson's correlation coefficient: {correlation:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "if p_value < 0.05:\n",
    "    print(\"The correlation is statistically significant.\")\n",
    "else:\n",
    "    print(\"The correlation is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords:\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize and remove stopwords:\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply to text column in DF:\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create frequency distribution for words:\n",
    "all_words = \" \".join(df['Cleaned_Text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display most common words:\n",
    "print(word_freq.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract entities:\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'LOC']]\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for Entity Recognition:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of Lovecraftian entities:\n",
    "lovecraftian_entities = [\n",
    "    \"cthulhu\", \"yog-sothoth\", \"nyarlathotep\", \"azathoth\", \"hastur\", \"r'lyeh\", \"dagon\", \n",
    "    \"shub-niggurath\", \"the great old ones\", \"elder gods\", \"the old ones\", \"the deep ones\", \"night gaunts\", \n",
    "    \"cthulhu cult\", \"the nameless city\", \"the black stone\", \"the dreamlands\", \"fenric\", \"hecuba\", \n",
    "    \"animus\", \"tor-gasukk\", \"moloch\", \"kai'lizakia\", \"lloigor\", \"eidolon\", \"derleth\", \"gog\", \"magog\", \"to'koth\", \n",
    "    \"karnas'koi\", \"traguam\", \"archon\", \"mi'en kalarash\", \"kwundaar\", \"volund\", \"k'thun\", \"noth-yidik\", \"tru'nembra\", \n",
    "    \"tulzscha\", \"cxaxukluth\", \"d'endrrah\", \"ubbo-sathla\", \"xexanoth\", \"ycnàgnnisssz\", \"yhoundeh\", \"aiueb gnshal\", \n",
    "    \"aletheia\", \"azhorra-tha\", \"c'thalpa\", \"daoloth\", \"ghroth\", \"gi-hoveg\", \"haiogh-yai\", \"huitloxopetl\", \"ialdagorth\", \n",
    "    \"kaajh'kaalbh\", \"kaalut\", \"lu-kthu\", \"mh'ithrha\", \"mlandoth\", \"mril thorion\", \"mother of pus\", \"nhimbaloth\", \n",
    "    \"ngyr-khorath\", \"nyctelios\", \"olkoth\", \"ramasekva\", \"shabbith-ka\", \"star mother\", \"suc'naath\", \"uvhash\", \"xa'ligha\", \n",
    "    \"yibb-tstll\", \"yidhra\", \"yomag'n'tho\"\n",
    "]\n",
    "\n",
    "# Indirect references or variations (these should be generalized and defined outside the loop for efficiency)\n",
    "indirect_references = [\n",
    "    r\"\\bdeep ones\\b\", r\"\\bcosmic entity\\b\", r\"\\bhorrible being\\b\", r\"\\bnight gaunts\\b\", r\"\\bblack stone\\b\",\n",
    "    r\"\\byog sothoth\\b\", r\"\\bnamesless city\\b\", r\"\\bstrange entity\\b\", r\"\\botherworldly creature\\b\", r\"\\bdark god\\b\",\n",
    "    r\"\\bhorrible power\\b\", r\"\\btimeless one\\b\"\n",
    "]\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize the list to store extracted entities\n",
    "    entities = []\n",
    "\n",
    "    # Manually add expanded entities and regex pattern matches for direct references\n",
    "    for entity in lovecraftian_entities:\n",
    "        # Check for direct entity mentions (singular and plural)\n",
    "        singular_entity = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        plural_entity = r'\\b' + re.escape(entity + \"s\") + r'\\b'  # Handle plural form\n",
    "\n",
    "        if re.search(singular_entity, text.lower()) or re.search(plural_entity, text.lower()):\n",
    "            entities.append(entity)\n",
    "        \n",
    "        # Check for indirect references or variations\n",
    "        for pattern in indirect_references:\n",
    "            if re.search(pattern, text.lower()):\n",
    "                entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Apply the extraction to the 'Cleaned_Text' column (assuming 'Cleaned_Text' contains the content)\n",
    "df['Entities'] = df['Cleaned_Text'].apply(extract_entities)\n",
    "\n",
    "# Flatten the list of entities and get their frequency count\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = Counter(all_entities)\n",
    "\n",
    "# Display the most common entities\n",
    "print(entity_freq.most_common(100))\n",
    "\n",
    "# Custom list of Lovecraftian entities to track (including new entities)\n",
    "specific_entity_freq = {entity: all_entities.count(entity) for entity in lovecraftian_entities}\n",
    "\n",
    "# Display the count of these specific entities\n",
    "print(specific_entity_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the mentions of each entity across all rows in 'Entities' column\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_mentions = Counter(all_entities)\n",
    "\n",
    "# Convert the Counter object to a list of tuples (entity, mentions)\n",
    "entities = [(entity, count) for entity, count in entity_mentions.items()]\n",
    "\n",
    "# Function to filter and categorize entities:\n",
    "def filter_lovecraft_entities(entities, exclude_humans=True):\n",
    "\n",
    "    # Dictionary to store entities by category:\n",
    "    categories = {\n",
    "        'Cthulhu Mythos': [],\n",
    "        'Locations & Settings': [],\n",
    "        'Cosmic Entities': [],\n",
    "        'Occult Entities': [],\n",
    "        'Mythos-Related Concepts': []\n",
    "    }\n",
    "\n",
    "    # Known classifications:\n",
    "    mythos_deities = {'cthulhu', 'great cthulhu', 'nyarlathotep', 'azathoth', 'shub-niggurath', \n",
    "                      'dagon', 'yog-sothoth', 'hastur', 'ubbo-sathla', 'ghroth', 'ycnàgnnisssz'}\n",
    "    \n",
    "    locations = {'arkham', 'miskatonic', 'innsmouth', 'dunwich', \"r'lyeh\", 'the dreamlands', 'the nameless city'}\n",
    "    \n",
    "    cosmic_entities = {'elder gods', 'the old ones', 'the great old ones', 'night gaunts', 'the deep ones', \n",
    "                       'colour out of space', 'yog sothoth', 'tulzscha', 'cxaxukluth', 'yhoundeh', \n",
    "                       'aiueb gnshal', 'aletheia', 'azhorra-tha', \"c'thalpa\", 'daoloth', 'ghroth', 'gi-hoveg', \n",
    "                       'haiogh-yai', 'huitloxopetl', 'ialdagorth', \"kaajh'kaalbh\", 'kaalut', 'lu-kthu', \n",
    "                       \"mh'ithrha\", 'mlandoth', 'mril thorion', 'mother of pus', 'nhimbaloth', \n",
    "                       'ngyr-khorath', 'nyctelios', 'olkoth', 'ramasekva', 'shabbith-ka', 'star mother', \n",
    "                       \"suc'naath\", 'uvhash', \"xa'ligha\", 'yibb-tstll', 'yidhra', \"yomag'n'tho\"}\n",
    "    \n",
    "    occult_entities = {'toymakers', 'guardians of time', 'the great intelligence', 'moloch', \n",
    "                       'hecuba', 'animus', 'archon', 'kwundaar', 'volund', 'noth-yidik', 'tru\\'nembra'}\n",
    "\n",
    "    # Categorize entities:\n",
    "    for entity, mentions in entities:\n",
    "\n",
    "        if entity in mythos_deities:\n",
    "            categories['Cthulhu Mythos'].append((entity, mentions))\n",
    "        elif entity in locations:\n",
    "            categories['Locations & Settings'].append((entity, mentions))\n",
    "        elif entity in cosmic_entities:\n",
    "            categories['Cosmic Entities'].append((entity, mentions))\n",
    "        elif entity in occult_entities:\n",
    "            categories['Occult Entities'].append((entity, mentions))\n",
    "        else:\n",
    "            categories['Mythos-Related Concepts'].append((entity, mentions))\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Apply filter:\n",
    "filtered_entities = filter_lovecraft_entities(entities)\n",
    "\n",
    "# Print result:\n",
    "for category, entities_list in filtered_entities.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for entity, mentions in entities_list:\n",
    "        print(f\"{entity}: {mentions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count mentions of each entity across all rows in Entities column:\n",
    "all_entities = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_mentions = Counter(all_entities)\n",
    "\n",
    "# Convert Counter object to DF (top 10):\n",
    "entity_df = pd.DataFrame(entity_mentions.most_common(10), columns=['Entity', 'Frequency'])\n",
    "\n",
    "# Plot top 10 entities:\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Frequency', y='Entity', data=entity_df, palette='viridis', hue='Entity')\n",
    "plt.title('Top 10 Lovecraftian Entities')\n",
    "plt.xlabel('Frequency of Mentions')\n",
    "plt.ylabel('Entity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Save entity frequencies to CSV:\n",
    "entity_df.to_csv('data/entity_frequencies.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot the most common words (word frequency analysis)\n",
    "top_words = word_freq.most_common(20)\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(words), y=list(counts))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot the most common entities\n",
    "top_entities = entity_freq.most_common(20)\n",
    "entities, entity_counts = zip(*top_entities)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(entities), y=list(entity_counts))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 20 Most Common Lovecraftian Entities')\n",
    "plt.xlabel('Entities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Beispiel-Datensatz erstellen\n",
    "data = {\n",
    "    'Title': ['The Call of Cthulhu', 'At the Mountains of Madness', 'The Dunwich Horror', 'The Shadow over Innsmouth'],\n",
    "    'Word_Count': [27000, 41000, 17500, 27000],\n",
    "    'Entities': [['Cthulhu', 'Rlyeh'], ['Elder Things', 'Shoggoths'], ['Yog-Sothoth', 'Whateley'], ['Deep Ones', 'Innsmouth']],\n",
    "    'Genre': ['Horror', 'Horror', 'Horror', 'Horror']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Verwendung von lambda und apply: Neue Spalten basierend auf bestehenden Spalten\n",
    "# Erstellen einer Spalte mit der Anzahl der Entities\n",
    "\n",
    "df['Entity_Count'] = df['Entities'].apply(lambda x: len(x))\n",
    "\n",
    "# 2. Groupby und Aggregation\n",
    "# Durchschnittliche Wortanzahl pro Genre\n",
    "avg_word_count = df.groupby('Genre')['Word_Count'].mean()\n",
    "\n",
    "# Gesamtanzahl der Entities pro Genre\n",
    "total_entities = df.groupby('Genre')['Entity_Count'].sum()\n",
    "\n",
    "# 3. Erstellung einer Häufigkeitstabelle\n",
    "# Zählen, wie oft eine bestimmte Entity auftaucht\n",
    "entity_list = [entity for sublist in df['Entities'] for entity in sublist]\n",
    "entity_freq = pd.Series(entity_list).value_counts()\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"\\nDataFrame mit neuer Spalte Entity_Count:\")\n",
    "print(df)\n",
    "print(\"\\nDurchschnittliche Wortanzahl pro Genre:\")\n",
    "print(avg_word_count)\n",
    "print(\"\\nGesamtanzahl der Entities pro Genre:\")\n",
    "print(total_entities)\n",
    "print(\"\\nHäufigkeitstabelle der Entities:\")\n",
    "print(entity_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
